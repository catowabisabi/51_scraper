"""
å°æ¯”èˆŠ/æ–°çˆ¬èŸ²çš„è³‡æ–™æå–æ•ˆæœ
æ‰¾å‡ºå“ªäº›æ¬„ä½æ˜¯èˆŠæ–¹æ³•èƒ½æå–ä½†æ–°æ–¹æ³•æå–ä¸åˆ°çš„
"""

import sys
import os
import sqlite3
from datetime import datetime

from bs4 import BeautifulSoup
import requests


# ============== è³‡æ–™åº«è·¯å¾‘ ==============
OLD_DB_PATH = os.path.join(os.path.dirname(__file__), "51ca-old.db")
NEW_DB_PATH = os.path.join(os.path.dirname(__file__), "scrapers", "data", "51ca.db")


def init_old_db():
    """åˆå§‹åŒ–èˆŠè³‡æ–™åº«"""
    conn = sqlite3.connect(OLD_DB_PATH)
    cursor = conn.cursor()
    
    # æ–°èè¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS news_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            article_id TEXT UNIQUE,
            url TEXT NOT NULL,
            title TEXT,
            summary TEXT,
            content TEXT,
            category TEXT,
            author TEXT,
            source TEXT,
            publish_date TIMESTAMP,
            image_urls TEXT,
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # æ±½è»Šè¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS auto_listings (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            listing_id TEXT UNIQUE,
            url TEXT NOT NULL,
            title TEXT,
            listing_type TEXT,
            make TEXT,
            model TEXT,
            year INTEGER,
            price REAL,
            mileage INTEGER,
            body_type TEXT,
            transmission TEXT,
            fuel_type TEXT,
            drivetrain TEXT,
            color TEXT,
            vin TEXT,
            description TEXT,
            features TEXT,
            seller_type TEXT,
            seller_name TEXT,
            contact_phone TEXT,
            location TEXT,
            post_date TEXT,
            image_urls TEXT,
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # æˆ¿å±‹è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS house_listings (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            listing_id TEXT UNIQUE,
            url TEXT NOT NULL,
            title TEXT,
            listing_type TEXT,
            property_type TEXT,
            price REAL,
            price_unit TEXT,
            address TEXT,
            city TEXT,
            community TEXT,
            bedrooms TEXT,
            bathrooms TEXT,
            parking TEXT,
            sqft TEXT,
            description TEXT,
            features TEXT,
            agent_name TEXT,
            agent_phone TEXT,
            agent_company TEXT,
            image_urls TEXT,
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # é›†å¸‚è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS market_posts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_id TEXT UNIQUE,
            url TEXT NOT NULL,
            title TEXT,
            description TEXT,
            price REAL,
            category TEXT,
            location TEXT,
            contact_phone TEXT,
            user_name TEXT,
            photos TEXT,
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    conn.commit()
    return conn


class OldStyleScraper:
    """æ¨¡æ“¬èˆŠçˆ¬èŸ²çš„æå–é‚è¼¯"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch(self, url):
        """ç²å–é é¢"""
        try:
            resp = self.session.get(url, timeout=30)
            resp.encoding = 'utf-8'
            return resp.text
        except Exception as e:
            print(f"ç²å–å¤±æ•—: {url} - {e}")
            return None
    
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return None
        import re
        text = re.sub(r'\s+', ' ', str(text)).strip()
        return text if text else None
    
    # ============== æ–°èè§£æï¼ˆèˆŠæ–¹æ³•ï¼‰==============
    def parse_news_old(self, html, url):
        """èˆŠæ–¹æ³•è§£ææ–°è"""
        soup = BeautifulSoup(html, "lxml")
        
        import re
        match = re.search(r'/articles/(\d+)', url)
        article_id = match.group(1) if match else None
        
        # æ¨™é¡Œ - å¤šç¨®é¸æ“‡å™¨
        title = None
        for selector in ['h1.article-title', 'h1', '.article-header h1', '#article-main h1']:
            elem = soup.select_one(selector)
            if elem:
                title = self.clean_text(elem.get_text())
                if title:
                    break
        
        # æ‘˜è¦
        summary = None
        summary_elem = soup.select_one('.article-summary, .summary, .lead, .article-intro')
        if summary_elem:
            summary = self.clean_text(summary_elem.get_text())
        
        # æ­£æ–‡
        content = None
        for selector in ['#arcbody', '.article-content', '.article-body', '#article-content']:
            elem = soup.select_one(selector)
            if elem:
                # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
                for tag in elem.find_all(['script', 'style', 'iframe']):
                    tag.decompose()
                content = self.clean_text(elem.get_text())
                if content and len(content) > 50:
                    break
        
        # åˆ†é¡
        category = None
        cat_elem = soup.select_one('.article-category, .category, .breadcrumb a:last-child')
        if cat_elem:
            category = self.clean_text(cat_elem.get_text())
        
        # ä¾†æº
        source = None
        source_elem = soup.select_one('.article-source, .source, .article-meta .source')
        if source_elem:
            source = self.clean_text(source_elem.get_text())
        
        # ä½œè€…
        author = None
        author_elem = soup.select_one('.article-author, .author, .byline')
        if author_elem:
            author = self.clean_text(author_elem.get_text())
        
        # ç™¼å¸ƒæ—¥æœŸ
        publish_date = None
        date_elem = soup.select_one('.article-date, .publish-time, .time, .article-meta time')
        if date_elem:
            publish_date = self.clean_text(date_elem.get_text())
        
        # åœ–ç‰‡
        import json
        images = []
        for img in soup.select('#arcbody img, .article-content img'):
            src = img.get('data-src') or img.get('src')
            if src and 'logo' not in src.lower():
                images.append(src)
        image_urls = json.dumps(images[:10]) if images else None
        
        return {
            'article_id': article_id,
            'url': url,
            'title': title,
            'summary': summary,
            'content': content,
            'category': category,
            'author': author,
            'source': source,
            'publish_date': publish_date,
            'image_urls': image_urls,
        }
    
    # ============== æ±½è»Šè§£æï¼ˆèˆŠæ–¹æ³•ï¼‰==============
    def parse_auto_old(self, html, url):
        """èˆŠæ–¹æ³•è§£ææ±½è»Š"""
        soup = BeautifulSoup(html, "lxml")
        
        import re
        match = re.search(r'/(\d+)$', url)
        listing_id = match.group(1) if match else None
        
        # æ¨™é¡Œ
        title = None
        title_elem = soup.find('h1')
        if title_elem:
            title = self.clean_text(title_elem.get_text())
            # æ¸…ç†æ¨™é¡Œå¾Œç¶´
            if title:
                title = re.sub(r'\s*[-|_].*51.*$', '', title)
        
        # é¡å‹
        listing_type = 'äºŒæ‰‹'
        if '/new-cars/' in url:
            listing_type = 'æ–°è»Š'
        elif '/lease-cars/' in url:
            listing_type = 'è½‰lease'
        
        # åƒ¹æ ¼ - å¤šç¨®æ¨¡å¼
        price = None
        price_patterns = [
            r'\$\s*([\d,]+)',
            r'([\d,]+)\s*\$',
            r'åƒ¹æ ¼[ï¼š:]\s*\$?([\d,]+)',
        ]
        text = soup.get_text()
        for pattern in price_patterns:
            match = re.search(pattern, text)
            if match:
                price_str = match.group(1).replace(',', '')
                try:
                    price = float(price_str)
                    if price > 100:  # æ’é™¤å¤ªå°çš„æ•¸å­—
                        break
                except:
                    pass
        
        # å¹´ä»½
        year = None
        if title:
            match = re.search(r'\b(19|20)\d{2}\b', title)
            if match:
                year = int(match.group(0))
        
        # å“ç‰Œå’Œå‹è™Ÿ
        make = None
        model = None
        brands = ['Toyota', 'Honda', 'Nissan', 'BMW', 'Mercedes', 'Audi', 'Lexus', 
                  'Ford', 'Chevrolet', 'Hyundai', 'Kia', 'Volkswagen', 'Mazda',
                  'Subaru', 'Jeep', 'Chrysler', 'Dodge', 'Porsche', 'Infiniti', 'Acura']
        if title:
            for brand in brands:
                if brand.lower() in title.lower():
                    make = brand
                    # å˜—è©¦æå–å‹è™Ÿ
                    pattern = rf'{brand}\s+(\w+)'
                    m = re.search(pattern, title, re.IGNORECASE)
                    if m:
                        model = m.group(1)
                    break
        
        # é‡Œç¨‹
        mileage = None
        mileage_patterns = [
            r'([\d,]+)\s*(?:km|å…¬é‡Œ|KM)',
            r'é‡Œç¨‹[ï¼š:]\s*([\d,]+)',
            r'Mileage[ï¼š:]\s*([\d,]+)',
        ]
        for pattern in mileage_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    mileage = int(match.group(1).replace(',', ''))
                    break
                except:
                    pass
        
        # è®Šé€Ÿç®±
        transmission = None
        if 'automatic' in text.lower() or 'è‡ªå‹•' in text:
            transmission = 'è‡ªå‹•'
        elif 'manual' in text.lower() or 'æ‰‹å‹•' in text:
            transmission = 'æ‰‹å‹•'
        
        # VIN
        vin = None
        vin_match = re.search(r'VIN[ï¼š:\s]*([A-HJ-NPR-Z0-9]{17})', text, re.IGNORECASE)
        if vin_match:
            vin = vin_match.group(1)
        
        # è³£å®¶ä¿¡æ¯
        seller_name = None
        seller_elem = soup.select_one('.dealer-name, .seller-name, .contact-name')
        if seller_elem:
            seller_name = self.clean_text(seller_elem.get_text())
        
        # é›»è©±
        contact_phone = None
        phone_match = re.search(r'(\d{3}[-.\s]?\d{3}[-.\s]?\d{4})', text)
        if phone_match:
            contact_phone = phone_match.group(1)
        
        # ä½ç½®
        location = None
        loc_elem = soup.select_one('.location, .address, .dealer-address')
        if loc_elem:
            location = self.clean_text(loc_elem.get_text())
        
        # æè¿°
        description = None
        desc_elem = soup.select_one('.description, .vehicle-description, .listing-description')
        if desc_elem:
            description = self.clean_text(desc_elem.get_text())
        
        # åœ–ç‰‡
        import json
        images = []
        for img in soup.find_all('img'):
            src = img.get('data-src') or img.get('src')
            if src and ('51img' in src or 'storage' in src) and 'logo' not in src.lower():
                images.append(src)
        image_urls = json.dumps(list(set(images))[:10]) if images else None
        
        return {
            'listing_id': listing_id,
            'url': url,
            'title': title,
            'listing_type': listing_type,
            'make': make,
            'model': model,
            'year': year,
            'price': price,
            'mileage': mileage,
            'transmission': transmission,
            'vin': vin,
            'seller_name': seller_name,
            'contact_phone': contact_phone,
            'location': location,
            'description': description,
            'image_urls': image_urls,
        }
    
    # ============== æˆ¿å±‹è§£æï¼ˆèˆŠæ–¹æ³•ï¼‰==============
    def parse_house_old(self, html, url):
        """èˆŠæ–¹æ³•è§£ææˆ¿å±‹"""
        soup = BeautifulSoup(html, "lxml")
        
        import re
        # æå– ID
        listing_id = None
        match = re.search(r'/(\d+)$', url)
        if match:
            listing_id = match.group(1)
        else:
            match = re.search(r'/property/([A-Z]\d+)', url)
            if match:
                listing_id = match.group(1)
        
        # æ¨™é¡Œ
        title = None
        title_elem = soup.find('h1')
        if title_elem:
            title = self.clean_text(title_elem.get_text())
        
        # é¡å‹
        listing_type = 'å‡ºå”®'
        if '/rental/' in url:
            listing_type = 'å‡ºç§Ÿ'
        
        # åƒ¹æ ¼
        price = None
        price_elem = soup.select_one('.price, .listing-price, .property-price')
        if price_elem:
            price_text = price_elem.get_text()
            match = re.search(r'\$?([\d,]+)', price_text)
            if match:
                try:
                    price = float(match.group(1).replace(',', ''))
                except:
                    pass
        
        # åœ°å€
        address = None
        addr_elem = soup.select_one('.address, .property-address, .listing-address')
        if addr_elem:
            address = self.clean_text(addr_elem.get_text())
        
        # åŸå¸‚
        city = None
        if '/toronto/' in url.lower():
            city = 'Toronto'
        elif '/markham/' in url.lower():
            city = 'Markham'
        elif '/vaughan/' in url.lower():
            city = 'Vaughan'
        elif '/richmond-hill/' in url.lower():
            city = 'Richmond Hill'
        elif '/mississauga/' in url.lower():
            city = 'Mississauga'
        
        # è‡¥å®¤/æµ´å®¤
        bedrooms = None
        bathrooms = None
        room_text = soup.get_text()
        bed_match = re.search(r'(\d+)\s*(?:bed|bedroom|è‡¥å®¤|æˆ¿)', room_text, re.IGNORECASE)
        if bed_match:
            bedrooms = bed_match.group(1)
        bath_match = re.search(r'(\d+)\s*(?:bath|bathroom|æµ´å®¤|è¡›)', room_text, re.IGNORECASE)
        if bath_match:
            bathrooms = bath_match.group(1)
        
        # æè¿°
        description = None
        desc_elem = soup.select_one('.description, .property-description, .listing-description')
        if desc_elem:
            description = self.clean_text(desc_elem.get_text())
        
        # åœ–ç‰‡
        import json
        images = []
        for img in soup.find_all('img'):
            src = img.get('data-src') or img.get('src')
            if src and ('51img' in src or 'storage' in src) and 'logo' not in src.lower():
                images.append(src)
        image_urls = json.dumps(list(set(images))[:10]) if images else None
        
        return {
            'listing_id': listing_id,
            'url': url,
            'title': title,
            'listing_type': listing_type,
            'price': price,
            'address': address,
            'city': city,
            'bedrooms': bedrooms,
            'bathrooms': bathrooms,
            'description': description,
            'image_urls': image_urls,
        }


def run_old_scraper(max_items=10):
    """é‹è¡ŒèˆŠçˆ¬èŸ²æå–è³‡æ–™"""
    print("=" * 60)
    print("ä½¿ç”¨èˆŠæ–¹æ³•çˆ¬å–è³‡æ–™åˆ° 51ca-old.db")
    print("=" * 60)
    
    conn = init_old_db()
    cursor = conn.cursor()
    scraper = OldStyleScraper()
    
    # ============== çˆ¬å–æ–°è ==============
    print("\n--- çˆ¬å–æ–°è ---")
    news_list_url = "https://info.51.ca/"
    html = scraper.fetch(news_list_url)
    if html:
        soup = BeautifulSoup(html, "lxml")
        import re
        links = soup.find_all('a', href=re.compile(r'/articles/\d+'))
        urls = list(set([f"https://info.51.ca{a['href'].split('?')[0]}" 
                        for a in links if a.get('href', '').startswith('/')]))[:max_items]
        
        for url in urls:
            print(f"  çˆ¬å–: {url}")
            html = scraper.fetch(url)
            if html:
                data = scraper.parse_news_old(html, url)
                try:
                    cursor.execute("""
                        INSERT OR REPLACE INTO news_articles 
                        (article_id, url, title, summary, content, category, author, source, publish_date, image_urls)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        data['article_id'], data['url'], data['title'], data['summary'],
                        data['content'], data['category'], data['author'], data['source'],
                        data['publish_date'], data['image_urls']
                    ))
                    conn.commit()
                    print(f"    âœ“ ä¿å­˜: {data['title'][:30] if data['title'] else 'N/A'}...")
                except Exception as e:
                    print(f"    âœ— éŒ¯èª¤: {e}")
    
    # ============== çˆ¬å–æ±½è»Š ==============
    print("\n--- çˆ¬å–æ±½è»Š ---")
    auto_list_url = "https://www.51.ca/autos/used-cars"
    html = scraper.fetch(auto_list_url)
    if html:
        soup = BeautifulSoup(html, "lxml")
        import re
        links = soup.find_all('a', href=re.compile(r'/autos/(used-cars|new-cars)/\d+'))
        urls = list(set([f"https://www.51.ca{a['href'].split('?')[0]}" 
                        for a in links if a.get('href', '').startswith('/')]))[:max_items]
        
        for url in urls:
            print(f"  çˆ¬å–: {url}")
            html = scraper.fetch(url)
            if html:
                data = scraper.parse_auto_old(html, url)
                try:
                    cursor.execute("""
                        INSERT OR REPLACE INTO auto_listings 
                        (listing_id, url, title, listing_type, make, model, year, price, 
                         mileage, transmission, vin, seller_name, contact_phone, location, 
                         description, image_urls)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        data['listing_id'], data['url'], data['title'], data['listing_type'],
                        data['make'], data['model'], data['year'], data['price'],
                        data['mileage'], data['transmission'], data['vin'], data['seller_name'],
                        data['contact_phone'], data['location'], data['description'], data['image_urls']
                    ))
                    conn.commit()
                    print(f"    âœ“ ä¿å­˜: {data['title'][:30] if data['title'] else 'N/A'}...")
                except Exception as e:
                    print(f"    âœ— éŒ¯èª¤: {e}")
    
    # ============== çˆ¬å–æˆ¿å±‹ ==============
    print("\n--- çˆ¬å–æˆ¿å±‹ ---")
    house_list_url = "https://house.51.ca/rental"
    html = scraper.fetch(house_list_url)
    if html:
        soup = BeautifulSoup(html, "lxml")
        import re
        links = soup.find_all('a', href=re.compile(r'/rental/ontario/[^/]+/[^/]+/\d+'))
        urls = list(set([f"https://house.51.ca{a['href'].split('?')[0]}" 
                        for a in links if a.get('href', '').startswith('/')]))[:max_items]
        
        for url in urls:
            print(f"  çˆ¬å–: {url}")
            html = scraper.fetch(url)
            if html:
                data = scraper.parse_house_old(html, url)
                try:
                    cursor.execute("""
                        INSERT OR REPLACE INTO house_listings 
                        (listing_id, url, title, listing_type, price, address, city, 
                         bedrooms, bathrooms, description, image_urls)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        data['listing_id'], data['url'], data['title'], data['listing_type'],
                        data['price'], data['address'], data['city'],
                        data['bedrooms'], data['bathrooms'], data['description'], data['image_urls']
                    ))
                    conn.commit()
                    print(f"    âœ“ ä¿å­˜: {data['title'][:30] if data['title'] else 'N/A'}...")
                except Exception as e:
                    print(f"    âœ— éŒ¯èª¤: {e}")
    
    conn.close()
    print("\nâœ“ èˆŠæ–¹æ³•çˆ¬å–å®Œæˆ!")
    return OLD_DB_PATH


def compare_databases():
    """å°æ¯”å…©å€‹è³‡æ–™åº«çš„è³‡æ–™è³ªé‡"""
    print("\n" + "=" * 60)
    print("å°æ¯”æ–°èˆŠçˆ¬èŸ²è³‡æ–™è³ªé‡")
    print("=" * 60)
    
    old_conn = sqlite3.connect(OLD_DB_PATH)
    new_conn = sqlite3.connect(NEW_DB_PATH)
    old_conn.row_factory = sqlite3.Row
    new_conn.row_factory = sqlite3.Row
    
    results = []
    
    # ============== å°æ¯”æ–°è ==============
    print("\nğŸ“° æ–°èå°æ¯”:")
    print("-" * 40)
    
    old_cur = old_conn.cursor()
    new_cur = new_conn.cursor()
    
    # ç²å–å…±åŒçš„ article_id
    old_cur.execute("SELECT article_id FROM news_articles")
    old_ids = set(r[0] for r in old_cur.fetchall())
    
    new_cur.execute("SELECT article_id FROM news_articles")
    new_ids = set(r[0] for r in new_cur.fetchall())
    
    common_ids = old_ids & new_ids
    print(f"  èˆŠDBæ–‡ç« æ•¸: {len(old_ids)}")
    print(f"  æ–°DBæ–‡ç« æ•¸: {len(new_ids)}")
    print(f"  å…±åŒæ–‡ç« æ•¸: {len(common_ids)}")
    
    if common_ids:
        news_fields = ['title', 'summary', 'content', 'category', 'author', 'source', 'publish_date', 'image_urls']
        
        for article_id in list(common_ids)[:5]:  # åªå°æ¯”å‰5å€‹
            old_cur.execute("SELECT * FROM news_articles WHERE article_id = ?", (article_id,))
            new_cur.execute("SELECT * FROM news_articles WHERE article_id = ?", (article_id,))
            
            old_row = dict(old_cur.fetchone())
            new_row = dict(new_cur.fetchone())
            
            print(f"\n  æ–‡ç«  {article_id}:")
            for field in news_fields:
                old_val = old_row.get(field)
                new_val = new_row.get(field)
                
                old_null = old_val is None or old_val == '' or old_val == 'null'
                new_null = new_val is None or new_val == '' or new_val == 'null'
                
                if old_null and not new_null:
                    status = "ğŸ†• æ–°æ–¹æ³•æœ‰"
                elif not old_null and new_null:
                    status = "âš ï¸ èˆŠæ–¹æ³•æœ‰"
                    results.append(('news', article_id, field, 'old_has_new_missing'))
                elif old_null and new_null:
                    status = "âŒ éƒ½æ²’æœ‰"
                else:
                    status = "âœ… éƒ½æœ‰"
                
                print(f"    {field:15}: {status}")
    
    # ============== å°æ¯”æ±½è»Š ==============
    print("\nğŸš— æ±½è»Šå°æ¯”:")
    print("-" * 40)
    
    old_cur.execute("SELECT listing_id FROM auto_listings")
    old_ids = set(r[0] for r in old_cur.fetchall())
    
    new_cur.execute("SELECT listing_id FROM auto_listings")
    new_ids = set(r[0] for r in new_cur.fetchall())
    
    common_ids = old_ids & new_ids
    print(f"  èˆŠDBæ±½è»Šæ•¸: {len(old_ids)}")
    print(f"  æ–°DBæ±½è»Šæ•¸: {len(new_ids)}")
    print(f"  å…±åŒæ±½è»Šæ•¸: {len(common_ids)}")
    
    if common_ids:
        auto_fields = ['title', 'make', 'model', 'year', 'price', 'mileage', 
                       'transmission', 'vin', 'seller_name', 'contact_phone', 'location', 'description']
        
        for listing_id in list(common_ids)[:5]:
            old_cur.execute("SELECT * FROM auto_listings WHERE listing_id = ?", (listing_id,))
            new_cur.execute("SELECT * FROM auto_listings WHERE listing_id = ?", (listing_id,))
            
            old_row = dict(old_cur.fetchone())
            new_row = dict(new_cur.fetchone())
            
            print(f"\n  æ±½è»Š {listing_id}:")
            for field in auto_fields:
                old_val = old_row.get(field)
                new_val = new_row.get(field)
                
                old_null = old_val is None or old_val == '' or old_val == 'null'
                new_null = new_val is None or new_val == '' or new_val == 'null'
                
                if old_null and not new_null:
                    status = "ğŸ†• æ–°æ–¹æ³•æœ‰"
                elif not old_null and new_null:
                    status = "âš ï¸ èˆŠæ–¹æ³•æœ‰"
                    results.append(('auto', listing_id, field, 'old_has_new_missing'))
                elif old_null and new_null:
                    status = "âŒ éƒ½æ²’æœ‰"
                else:
                    status = "âœ… éƒ½æœ‰"
                
                print(f"    {field:15}: {status}")
    
    # ============== å°æ¯”æˆ¿å±‹ ==============
    print("\nğŸ  æˆ¿å±‹å°æ¯”:")
    print("-" * 40)
    
    old_cur.execute("SELECT listing_id FROM house_listings")
    old_ids = set(r[0] for r in old_cur.fetchall())
    
    new_cur.execute("SELECT listing_id FROM house_listings")
    new_ids = set(r[0] for r in new_cur.fetchall())
    
    common_ids = old_ids & new_ids
    print(f"  èˆŠDBæˆ¿å±‹æ•¸: {len(old_ids)}")
    print(f"  æ–°DBæˆ¿å±‹æ•¸: {len(new_ids)}")
    print(f"  å…±åŒæˆ¿å±‹æ•¸: {len(common_ids)}")
    
    if common_ids:
        house_fields = ['title', 'price', 'address', 'city', 'bedrooms', 'bathrooms', 'description']
        
        for listing_id in list(common_ids)[:5]:
            old_cur.execute("SELECT * FROM house_listings WHERE listing_id = ?", (listing_id,))
            new_cur.execute("SELECT * FROM house_listings WHERE listing_id = ?", (listing_id,))
            
            old_row = dict(old_cur.fetchone())
            new_row = dict(new_cur.fetchone())
            
            print(f"\n  æˆ¿å±‹ {listing_id}:")
            for field in house_fields:
                old_val = old_row.get(field)
                new_val = new_row.get(field)
                
                old_null = old_val is None or old_val == '' or old_val == 'null'
                new_null = new_val is None or new_val == '' or new_val == 'null'
                
                if old_null and not new_null:
                    status = "ğŸ†• æ–°æ–¹æ³•æœ‰"
                elif not old_null and new_null:
                    status = "âš ï¸ èˆŠæ–¹æ³•æœ‰"
                    results.append(('house', listing_id, field, 'old_has_new_missing'))
                elif old_null and new_null:
                    status = "âŒ éƒ½æ²’æœ‰"
                else:
                    status = "âœ… éƒ½æœ‰"
                
                print(f"    {field:15}: {status}")
    
    old_conn.close()
    new_conn.close()
    
    # ============== ç¸½çµ ==============
    print("\n" + "=" * 60)
    print("ç¸½çµ: èˆŠæ–¹æ³•æœ‰ä½†æ–°æ–¹æ³•ç¼ºå¤±çš„æ¬„ä½")
    print("=" * 60)
    
    if results:
        for table, item_id, field, _ in results:
            print(f"  [{table}] {item_id}: {field}")
    else:
        print("  æ²’æœ‰ç™¼ç¾èˆŠæ–¹æ³•ç¨æœ‰çš„è³‡æ–™")
    
    return results


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='å°æ¯”æ–°èˆŠçˆ¬èŸ²')
    parser.add_argument('--max', type=int, default=10, help='æ¯é¡æœ€å¤šçˆ¬å–æ•¸é‡')
    parser.add_argument('--compare-only', action='store_true', help='åªå°æ¯”ï¼Œä¸çˆ¬å–')
    args = parser.parse_args()
    
    if not args.compare_only:
        run_old_scraper(max_items=args.max)
    
    compare_databases()
