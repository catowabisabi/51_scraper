"""
51.ca ç¶²ç«™çˆ¬èŸ²
ä½¿ç”¨ Playwright çˆ¬å–å‹•æ…‹å…§å®¹ï¼Œå°‡è³‡æ–™å­˜å…¥ SQLite
"""

import sqlite3
import time
import re
from urllib.parse import urljoin, urlparse
from datetime import datetime
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup


# ============== è³‡æ–™åº«è¨­å®š ==============
DB_PATH = "51ca_data.db"


def init_db():
    """åˆå§‹åŒ– SQLite è³‡æ–™åº«"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # å‰µå»ºé é¢è³‡æ–™è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS pages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE NOT NULL,
            title TEXT,
            content TEXT,
            html TEXT,
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # å‰µå»ºé€£çµè³‡æ–™è¡¨ï¼ˆç”¨æ–¼è¿½è¹¤çˆ¬å–é€²åº¦ï¼‰
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS links (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE NOT NULL,
            source_url TEXT,
            visited INTEGER DEFAULT 0,
            added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    conn.commit()
    conn.close()
    print("âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")


def add_link(url, source_url=None):
    """æ·»åŠ é€£çµåˆ°è³‡æ–™åº«"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    try:
        cursor.execute(
            "INSERT OR IGNORE INTO links (url, source_url) VALUES (?, ?)",
            (url, source_url)
        )
        conn.commit()
    except Exception as e:
        print(f"æ·»åŠ é€£çµå¤±æ•—: {e}")
    finally:
        conn.close()


def mark_visited(url):
    """æ¨™è¨˜é€£çµç‚ºå·²è¨ªå•"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("UPDATE links SET visited = 1 WHERE url = ?", (url,))
    conn.commit()
    conn.close()


def get_unvisited_links(limit=10):
    """ç²å–æœªè¨ªå•çš„é€£çµ"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute(
        "SELECT url FROM links WHERE visited = 0 LIMIT ?",
        (limit,)
    )
    urls = [row[0] for row in cursor.fetchall()]
    conn.close()
    return urls


def save_page(url, title, content, html):
    """ä¿å­˜é é¢è³‡æ–™åˆ°è³‡æ–™åº«"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    try:
        cursor.execute(
            """INSERT OR REPLACE INTO pages (url, title, content, html, scraped_at) 
               VALUES (?, ?, ?, ?, ?)""",
            (url, title, content, html, datetime.now())
        )
        conn.commit()
        print(f"âœ… å·²ä¿å­˜: {title[:50]}...")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±æ•—: {e}")
    finally:
        conn.close()


def is_valid_url(url, base_domain="51.ca"):
    """æª¢æŸ¥URLæ˜¯å¦æœ‰æ•ˆä¸”å±¬æ–¼ç›®æ¨™ç¶²åŸŸ"""
    if not url:
        return False
    
    parsed = urlparse(url)
    
    # æ’é™¤éHTTPå”è­°
    if parsed.scheme not in ("http", "https", ""):
        return False
    
    # æ’é™¤ç‰¹å®šæ–‡ä»¶é¡å‹
    excluded_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.rar', '.exe', '.mp3', '.mp4')
    if parsed.path.lower().endswith(excluded_extensions):
        return False
    
    # ç¢ºä¿æ˜¯ç›®æ¨™ç¶²åŸŸ
    if parsed.netloc and base_domain not in parsed.netloc:
        return False
    
    return True


def extract_links(html, base_url):
    """å¾HTMLä¸­æå–æ‰€æœ‰é€£çµ"""
    soup = BeautifulSoup(html, "lxml")
    links = set()
    
    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        full_url = urljoin(base_url, href)
        
        # æ¸…ç†URLï¼ˆç§»é™¤fragmentï¼‰
        parsed = urlparse(full_url)
        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if parsed.query:
            clean_url += f"?{parsed.query}"
        
        if is_valid_url(clean_url):
            links.add(clean_url)
    
    return links


def extract_content(html):
    """å¾HTMLä¸­æå–æ–‡å­—å…§å®¹"""
    soup = BeautifulSoup(html, "lxml")
    
    # ç§»é™¤scriptå’Œstyleæ¨™ç±¤
    for tag in soup(["script", "style", "nav", "footer", "header"]):
        tag.decompose()
    
    # ç²å–ç´”æ–‡å­—
    text = soup.get_text(separator="\n", strip=True)
    
    # æ¸…ç†å¤šé¤˜ç©ºç™½
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    return text


def crawl_page(page, url):
    """çˆ¬å–å–®ä¸€é é¢"""
    try:
        print(f"ğŸ” æ­£åœ¨çˆ¬å–: {url}")
        
        # è¨ªå•é é¢
        page.goto(url, timeout=30000, wait_until="domcontentloaded")
        time.sleep(2)  # ç­‰å¾…å‹•æ…‹å…§å®¹åŠ è¼‰
        
        # ç²å–é é¢å…§å®¹
        html = page.content()
        title = page.title()
        content = extract_content(html)
        
        # ä¿å­˜é é¢
        save_page(url, title, content, html)
        
        # æå–é€£çµ
        links = extract_links(html, url)
        print(f"ğŸ“ ç™¼ç¾ {len(links)} å€‹é€£çµ")
        
        # æ·»åŠ æ–°é€£çµåˆ°è³‡æ–™åº«
        for link in links:
            add_link(link, url)
        
        # æ¨™è¨˜ç•¶å‰é é¢ç‚ºå·²è¨ªå•
        mark_visited(url)
        
        return True
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±æ•— {url}: {e}")
        mark_visited(url)  # é¿å…é‡è¤‡å˜—è©¦å¤±æ•—çš„é é¢
        return False


def crawl_homepage():
    """çˆ¬å–ä¸»é ä¸¦é¡¯ç¤ºå…§å®¹"""
    print("=" * 60)
    print("ğŸš€ é–‹å§‹çˆ¬å– 51.ca ä¸»é ")
    print("=" * 60)
    
    # åˆå§‹åŒ–è³‡æ–™åº«
    init_db()
    
    with sync_playwright() as p:
        # å•Ÿå‹•ç€è¦½å™¨
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        # çˆ¬å–ä¸»é 
        homepage_url = "https://www.51.ca/"
        add_link(homepage_url, None)
        
        success = crawl_page(page, homepage_url)
        
        if success:
            # é¡¯ç¤ºçµ±è¨ˆ
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM links")
            total_links = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM links WHERE visited = 1")
            visited_links = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM pages")
            total_pages = cursor.fetchone()[0]
            
            print("\n" + "=" * 60)
            print("ğŸ“Š çˆ¬å–çµ±è¨ˆ:")
            print(f"   - ç™¼ç¾é€£çµ: {total_links}")
            print(f"   - å·²è¨ªå•: {visited_links}")
            print(f"   - å·²ä¿å­˜é é¢: {total_pages}")
            print("=" * 60)
            
            # é¡¯ç¤ºéƒ¨åˆ†é€£çµ
            cursor.execute("SELECT url FROM links LIMIT 20")
            links = cursor.fetchall()
            print("\nğŸ”— ç™¼ç¾çš„é€£çµ (å‰20å€‹):")
            for i, (link,) in enumerate(links, 1):
                print(f"   {i}. {link}")
            
            conn.close()
        
        browser.close()
    
    print("\nâœ… ä¸»é çˆ¬å–å®Œæˆï¼è³‡æ–™å·²å­˜å…¥ 51ca_data.db")


def continue_crawling(max_pages=100):
    """ç¹¼çºŒçˆ¬å–æœªè¨ªå•çš„é é¢"""
    print("=" * 60)
    print(f"ğŸ”„ ç¹¼çºŒçˆ¬å– (æœ€å¤š {max_pages} é )")
    print("=" * 60)
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        page = context.new_page()
        
        pages_crawled = 0
        
        while pages_crawled < max_pages:
            urls = get_unvisited_links(limit=5)
            
            if not urls:
                print("âœ… æ²’æœ‰æ›´å¤šæœªè¨ªå•çš„é€£çµ")
                break
            
            for url in urls:
                if pages_crawled >= max_pages:
                    break
                    
                crawl_page(page, url)
                pages_crawled += 1
                time.sleep(1)  # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
        
        browser.close()
    
    print(f"\nâœ… æœ¬æ¬¡çˆ¬å–å®Œæˆï¼å…±çˆ¬å– {pages_crawled} é ")


if __name__ == "__main__":
    # å…ˆçˆ¬å–ä¸»é 
    crawl_homepage()
    
    # å¦‚æœæƒ³ç¹¼çºŒçˆ¬å–æ›´å¤šé é¢ï¼Œå–æ¶ˆä¸‹é¢çš„è¨»é‡‹
    # continue_crawling(max_pages=50)
