"""
51.ca ä¸»çˆ¬èŸ²ç¨‹å¼
çµ±ä¸€é‹è¡Œæ‰€æœ‰çˆ¬èŸ²
"""

import sys
import argparse
from datetime import datetime

import os
import sys

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, os.path.dirname(__file__))

from models import init_database, get_stats
from base_scraper import setup_logger

# å‹•æ…‹å°å…¥çˆ¬èŸ²é¡
def get_scrapers():
    """å‹•æ…‹ç²å–çˆ¬èŸ²é¡"""
    from importlib import import_module
    
    news_module = import_module('51_scraper_news')
    house_module = import_module('51_scraper_house')
    jobs_module = import_module('51_scraper_jobs')
    service_module = import_module('51_scraper_service')
    market_module = import_module('51_scraper_market')
    auto_module = import_module('51_scraper_auto')
    
    return {
        'news': (news_module.NewsScraper, "run_news_scraper"),
        'house': (house_module.HouseScraper, "run_house_scraper"),
        'jobs': (jobs_module.JobsScraper, "run_jobs_scraper"),
        'service': (service_module.ServiceScraper, "run_service_scraper"),
        'market': (market_module.MarketScraper, "run_market_scraper"),
        'auto': (auto_module.AutoScraper, "run_auto_scraper"),
    }


def run_all_scrapers(max_pages: int = 30, headless: bool = True):
    """é‹è¡Œæ‰€æœ‰çˆ¬èŸ²"""
    logger = setup_logger("main")
    
    logger.info("=" * 70)
    logger.info("51.ca å…¨ç«™çˆ¬èŸ²é–‹å§‹é‹è¡Œ")
    logger.info(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 70)
    
    # åˆå§‹åŒ–è³‡æ–™åº«
    init_database()
    
    scrapers_dict = get_scrapers()
    scrapers = [
        ("æ–°èçˆ¬èŸ²", scrapers_dict['news'][0], scrapers_dict['news'][1]),
        ("æˆ¿å±‹çˆ¬èŸ²", scrapers_dict['house'][0], scrapers_dict['house'][1]),
        ("å·¥ä½œçˆ¬èŸ²", scrapers_dict['jobs'][0], scrapers_dict['jobs'][1]),
        ("é»ƒé æœå‹™çˆ¬èŸ²", scrapers_dict['service'][0], scrapers_dict['service'][1]),
        ("é›†å¸‚çˆ¬èŸ²", scrapers_dict['market'][0], scrapers_dict['market'][1]),
        ("æ±½è»Šçˆ¬èŸ²", scrapers_dict['auto'][0], scrapers_dict['auto'][1]),
    ]
    
    for name, ScraperClass, run_method in scrapers:
        logger.info(f"\n{'=' * 50}")
        logger.info(f"é–‹å§‹é‹è¡Œ: {name}")
        logger.info(f"{'=' * 50}")
        
        try:
            scraper = ScraperClass(headless=headless)
            getattr(scraper, run_method)(max_pages=max_pages)
        except Exception as e:
            logger.error(f"{name} é‹è¡Œå¤±æ•—: {e}")
    
    # æ‰“å°çµ±è¨ˆ
    print_final_stats(logger)


def run_single_scraper(scraper_name: str, max_pages: int = 30, headless: bool = True):
    """é‹è¡Œå–®å€‹çˆ¬èŸ²"""
    logger = setup_logger("main")
    
    scrapers = get_scrapers()
    
    if scraper_name not in scrapers:
        logger.error(f"æœªçŸ¥çš„çˆ¬èŸ²: {scraper_name}")
        logger.info(f"å¯ç”¨çš„çˆ¬èŸ²: {', '.join(scrapers.keys())}")
        return
    
    ScraperClass, run_method = scrapers[scraper_name]
    
    logger.info(f"é‹è¡Œ {scraper_name} çˆ¬èŸ²")
    
    try:
        scraper = ScraperClass(headless=headless)
        getattr(scraper, run_method)(max_pages=max_pages)
    except Exception as e:
        logger.error(f"çˆ¬èŸ²é‹è¡Œå¤±æ•—: {e}")
    
    print_final_stats(logger)


def print_final_stats(logger):
    """æ‰“å°æœ€çµ‚çµ±è¨ˆ"""
    stats = get_stats()
    
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆ:")
    logger.info("-" * 40)
    logger.info(f"  ğŸ“° æ–°èæ–‡ç« : {stats.get('news_articles', 0)} ç¯‡")
    logger.info(f"  ğŸ  æˆ¿å±‹åˆ—è¡¨: {stats.get('house_listings', 0)} æ¢")
    logger.info(f"  ğŸ’¼ å·¥ä½œè·ä½: {stats.get('job_listings', 0)} å€‹")
    logger.info(f"  ğŸª é»ƒé å•†å®¶: {stats.get('service_merchants', 0)} å®¶")
    logger.info(f"  ğŸ“¦ é›†å¸‚å¸–å­: {stats.get('market_posts', 0)} æ¢")
    logger.info(f"  ğŸš— æ±½è»Šåˆ—è¡¨: {stats.get('auto_listings', 0)} æ¢")
    logger.info("-" * 40)
    logger.info(f"  â³ å¾…çˆ¬å–URL: {stats.get('pending_urls', 0)}")
    logger.info(f"  âœ… å·²çˆ¬å–URL: {stats.get('visited_urls', 0)}")
    logger.info("=" * 70)


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='51.ca ç¶²ç«™çˆ¬èŸ²')
    parser.add_argument('--scraper', '-s', type=str, default='all',
                        choices=['all', 'news', 'house', 'jobs', 'service', 'market', 'auto'],
                        help='è¦é‹è¡Œçš„çˆ¬èŸ² (é»˜èª: all)')
    parser.add_argument('--pages', '-p', type=int, default=30,
                        help='æœ€å¤§çˆ¬å–é æ•¸ (é»˜èª: 30)')
    parser.add_argument('--show', action='store_true',
                        help='é¡¯ç¤ºç€è¦½å™¨è¦–çª—')
    
    args = parser.parse_args()
    
    headless = not args.show
    
    if args.scraper == 'all':
        run_all_scrapers(max_pages=args.pages, headless=headless)
    else:
        run_single_scraper(args.scraper, max_pages=args.pages, headless=headless)


if __name__ == "__main__":
    main()
